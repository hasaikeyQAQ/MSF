# MVSS-Baseline
This repository provides the source code of MVNet baseline for MVSS task.

## Installation 

The code requires `python>=3.7`, as well as `pytorch>=1.9` and `torchvision>=0.10`. Please follow the instructions [here](https://pytorch.org/get-started/locally/) to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended. Then, please clone this repo.

1. **Clone this repo.**

   ```shell
   $ git clone https://github.com/DVSOD/DVSOD-Baseline.git
   $ cd MVNet-main
   ```

2. **Install dependencies.**

   ```shell
   $ conda create -n MVSS
   $ conda activate MVSS
   $ conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
   $ pip install scipy
   $ pip install tqdm
   $ pip install visdom
   $ pip install matplotlib
   ```

## Getting Started

First download the `MVSeg` [dataset](https://github.com/DVSOD/DVSOD-DViSal) and pretrained [model ckpt](xxx). Then the model can be used in just a few adaptions to start training:

1. Set your MVSeg dataset path in `datasets/mvss_dataset.py` and put the ckpt in file `save/*`.
2. Perform training from scratch, with ```bash train.sh``` in two stage. First for warm-up, second for MVNet.   
**or** Perform training based on our warm-up ckpt, with ```bash train.sh``` for second stage.

Meanwhile, the segmentation maps can be generated by loading the pretrained [model ckpt](xxx), with:
1. Set your MVSeg dataset path in `datasets/mvss_dataset.py` and put the ckpt in file `save/*`.
2. Specify testset name in `test.sh`, e.g., `--split-mode test`, or `--split-mode test_night`.
3. Perform inference, with ```bash test.sh```

Instructions for vital parameters in ```main.py```:
```
- set '--is_ResNet'      as **bool**         # whether use ResNet or not
- set '--ckpt_load'      as **bool**         # whether load checkpoint or not
- set '--snapshot'       as **int**          # e.g. 100, which means loading the 100th checkpoint
- set '--baseline_mode'  as **bool**         # whether apply baseline mode or not
- set '--sample_rate''   as **int**          # e.g. 3, whcih means sample rate
- set '--stm_queue_size' as **int**          # e.g. 3, whcih means the number of memory frames
- set '--batchsize'      as **int**          # e.g. 2, whcih means batch size
- set '--trainsize'      as **int**          # e.g. 320, whcih means training data size
- set '--save_interval'  as **int**          # e.g. 2, whcih means saving ckpt per 2 epochs
- set '--epoch'          as **int**          # e.g. 200, whcih means epoch number during training
- set '--lr'             as **float**        # e.g. 1e-4, whcih means learning rate
```

## Citation

```
@inproceedings{ji2023multispectral,
  title={Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline},
  author={Ji, Wei and Li, Jingjing and Bian, Cheng and Zhou, Zongwei and Zhao, Jiaying and Yuille, Alan L and Cheng, Li},
  booktitle={CVPR},
  pages={1094--1104},
  year={2023}
}
```


## Acknowledgement

This repository was originally built from [LMANet](https://github.com/mattpfr/lmanet). It was modified and extended to support our multispectral video setting.
